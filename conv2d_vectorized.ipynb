{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution 2D with vector operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "  A(input_batch)\n",
    "  B(U)\n",
    "  C(U_permuted)\n",
    "  D(U_reshaped)\n",
    "  E(kernel)\n",
    "  F(kernel_reshaped)\n",
    "  G(kernel_permuted)\n",
    "  H(output_2D)\n",
    "  I(output_3D)\n",
    "  J(output_4D)\n",
    "  K(output)\n",
    "\n",
    "  L((unfold))\n",
    "  M((permute_U))\n",
    "  N((reshape_U_permuted))\n",
    "  O((reshape_kernel))\n",
    "  T((permute_kernel_reshaped))\n",
    "  P((compute_output_2D))\n",
    "  Q((reshape_output_to_3D))\n",
    "  R((reshape_output_to_4D))\n",
    "  S((permute_output))\n",
    "\n",
    "  subgraph input_batch\n",
    "    A --> L --> B\n",
    "    B --> M --> C\n",
    "    C --> N --> D\n",
    "  end\n",
    "\n",
    "  subgraph kernel\n",
    "    E --> O --> F\n",
    "    F --> T --> G\n",
    "  end\n",
    "\n",
    "  D --> P\n",
    "  G --> P\n",
    "  P --> H\n",
    "  subgraph output\n",
    "    H --> Q --> I\n",
    "    I --> R --> J\n",
    "    J --> S --> K\n",
    "  end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batches = 4\n",
    "in_channels = 3\n",
    "i_h = 8\n",
    "i_w = 8\n",
    "out_channels = 3\n",
    "k_h = 3\n",
    "k_w = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "\n",
    "\n",
    "def unfold(\n",
    "    input_batch: torch.Tensor, kernel_size: tuple, stride: int, padding: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    input:\n",
    "        input_batch: (b, in_channels, i_h, i_w)\n",
    "    output:\n",
    "        (b, patch_size, patches)\n",
    "    \"\"\"\n",
    "\n",
    "    torch_unfold = torch.nn.Unfold(\n",
    "        kernel_size=kernel_size, padding=padding, stride=stride\n",
    "    )\n",
    "\n",
    "    return torch_unfold(input_batch)\n",
    "\n",
    "\n",
    "def permute_U(U: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    input:\n",
    "        input_batch: (b, patch_size, patches)\n",
    "    output:\n",
    "        (b, patches, patch_size)\n",
    "    \"\"\"\n",
    "\n",
    "    return U.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "def reshape_U_permuted(U_permuted: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    input:\n",
    "        input_batch: (b, patches, patch_size)\n",
    "    output:\n",
    "        (b * patches, patch_size)\n",
    "    \"\"\"\n",
    "    b, patches, patch_size = U_permuted.shape\n",
    "    return U_permuted.reshape(b * patches, patch_size)\n",
    "\n",
    "\n",
    "def reshape_kernel(kernel: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    input:\n",
    "        input_batch: (out_channels, in_channels, kernel_h, kernel_w)\n",
    "    output:\n",
    "        (out_channels, in_channels * kernel_h * kernel_w)\n",
    "    \"\"\"\n",
    "    out_channels, in_channels, kernel_h, kernel_w = kernel.shape\n",
    "    return kernel.reshape(out_channels, in_channels * kernel_h * kernel_w)\n",
    "\n",
    "\n",
    "def permute_kernel_reshaped(kernel_reshaped: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    input:\n",
    "        input_batch: (out_channels, in_channels * kernel_h * kernel_w)\n",
    "    output:\n",
    "        (in_channels * kernel_h * kernel_w, out_channels)\n",
    "    \"\"\"\n",
    "    return kernel_reshaped.permute(1, 0)\n",
    "\n",
    "\n",
    "def compute_output_2D(\n",
    "    U_reshaped: torch.Tensor, kernel_permuted: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    input:\n",
    "        U_reshaped: (b * patches, patch_size)\n",
    "        kernel_permuted: (in_channels * kernel_h * kernel_w, out_channels)\n",
    "    output:\n",
    "        (b * patches, out_channels)\n",
    "    \"\"\"\n",
    "    return U_reshaped.matmul(kernel_permuted)\n",
    "\n",
    "\n",
    "def reshape_output_to_3D(\n",
    "    output_2D: torch.Tensor, b: int, patches: int, out_channels: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    <<<<<:\n",
    "        output: (b * patches, out_channels)\n",
    "    >>>>>:\n",
    "        (b, patches, out_channels)\n",
    "    \"\"\"\n",
    "    return output_2D.reshape(b, patches, out_channels)\n",
    "\n",
    "\n",
    "def reshape_output_to_4D(\n",
    "    output_3D: torch.Tensor, b: int, o_h: int, o_w: int, out_channels: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    <<<<<:\n",
    "        output: (b, patches, out_channels)\n",
    "\n",
    "    >>>>>:\n",
    "        (b, o_h, o_w, out_channels)\n",
    "    \"\"\"\n",
    "    return output_3D.reshape(b, o_h, o_w, out_channels)\n",
    "\n",
    "\n",
    "def permute_output(output: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    <<<<<:\n",
    "        output: (b, o_h, o_w, out_channels)\n",
    "\n",
    "    >>>>>:\n",
    "        (b, out_channels, o_h, o_w)\n",
    "    \"\"\"\n",
    "    return output.permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "def get_output_dim(input_dim, filter_size, stride, padding):\n",
    "    return (input_dim - filter_size + 2 * padding) // stride + 1\n",
    "\n",
    "\n",
    "class Conv2DFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_batch, kernel, stride=1, padding=1):\n",
    "        ctx.stride = stride\n",
    "        ctx.padding = padding\n",
    "\n",
    "        b, in_channels, i_h, i_w = input_batch.shape\n",
    "        out_channels, in_channels, kernel_h, kernel_w = kernel.shape\n",
    "\n",
    "        o_h = get_output_dim(i_h, kernel_h, stride, padding)\n",
    "        o_w = get_output_dim(i_w, kernel_w, stride, padding)\n",
    "\n",
    "        U = unfold(\n",
    "            input_batch=input_batch,\n",
    "            kernel_size=(kernel_h, kernel_w),\n",
    "            padding=padding,\n",
    "            stride=stride,\n",
    "        )\n",
    "        b, patch_size, patches = U.shape\n",
    "\n",
    "        U_permuted = permute_U(U)\n",
    "        U_reshaped = reshape_U_permuted(U_permuted)\n",
    "\n",
    "        kernel_reshaped = reshape_kernel(kernel)\n",
    "        kernel_permuted = permute_kernel_reshaped(kernel_reshaped)\n",
    "\n",
    "        output_2D = compute_output_2D(U_reshaped, kernel_permuted)\n",
    "        output_3D = reshape_output_to_3D(output_2D, b, patches, out_channels)\n",
    "        output_4D = reshape_output_to_4D(output_3D, b, o_h, o_w, out_channels)\n",
    "        output = permute_output(output_4D)\n",
    "\n",
    "        ctx.save_for_backward(input_batch, kernel, U_reshaped, kernel_permuted)\n",
    "        return ctx, output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_batch, kernel, U_reshaped, kernel_permuted = ctx.saved_tensors\n",
    "        b, in_channels, i_h, i_w = input_batch.shape\n",
    "        out_channels, in_channels, kernel_h, kernel_w = kernel.shape\n",
    "        b, out_channels, o_h, o_w = grad_output.shape\n",
    "        patches = o_h * o_w\n",
    "\n",
    "        stride, padding = ctx.stride, ctx.padding\n",
    "\n",
    "        # backward for `permute_output`\n",
    "        # (b, out_channels, o_h, o_w) -> (b, o_h, o_w, out_channels)\n",
    "        output_4D_grad = grad_output.permute(0, 2, 3, 1)\n",
    "\n",
    "        # backward for `reshape_output_to_4D`\n",
    "        # (b, o_h, o_w, out_channels) -> (b, patches, out_channels)\n",
    "        output_3D_grad = output_4D_grad.reshape(b, o_h * o_w, out_channels)\n",
    "\n",
    "        # backward for `reshape_output_to_3D`\n",
    "        # (b, patches, out_channels) -> (b * patches, out_channels)\n",
    "        output_2D_grad = output_3D_grad.reshape(b * patches, out_channels)\n",
    "\n",
    "        # ----------------- kernel gradient ----------------------------------------------\n",
    "        # backward of `compute_output_2D` with respect to kernel_permuted\n",
    "        # U_reshaped of shape (b * patches, patch_size)\n",
    "        # output_2D_grad of shape (b * patches, out_channels)\n",
    "        # kernel_permuted of shape (in_channels * kernel_h * kernel_w, out_channels)\n",
    "        kernel_permuted_grad = torch.matmul(U_reshaped.t(), output_2D_grad)\n",
    "        assert kernel_permuted_grad.shape == (\n",
    "            in_channels * kernel_h * kernel_w,\n",
    "            out_channels,\n",
    "        )\n",
    "\n",
    "        # backward of `permute_kernel_reshaped`\n",
    "        # (in_channels * kernel_h * kernel_w, out_channels)\n",
    "        #   -> (out_channels, in_channels * kernel_h * kernel_w)\n",
    "        kernel_reshaped_grad = kernel_permuted_grad.permute(1, 0)\n",
    "\n",
    "        # backward of `reshape_kernel`\n",
    "        # (out_channels, in_channels * kernel_h * kernel_w)\n",
    "        #   -> (out_channels, in_channels, kernel_h, kernel_w)\n",
    "        kernel_grad = kernel_reshaped_grad.reshape(\n",
    "            out_channels, in_channels, kernel_h, kernel_w\n",
    "        )\n",
    "        assert kernel_grad.shape == (out_channels, in_channels, kernel_h, kernel_w)\n",
    "\n",
    "        # ----------------- input_batch gradient -----------------------------------------\n",
    "        # backward of `compute_output_2D` with respect to U_reshaped\n",
    "        # kernel_permuted of shape (in_channels * kernel_h * kernel_w, out_channels)\n",
    "        # output_2D_grad of shape (b * patches, out_channels)\n",
    "        # U_reshaped of shape (b * patches, patch_size)\n",
    "        patch_size = in_channels * kernel_h * kernel_w\n",
    "        U_reshaped_grad = torch.matmul(output_2D_grad, kernel_permuted.t())\n",
    "        assert U_reshaped_grad.shape == (b * patches, patch_size)\n",
    "\n",
    "        # backward of `reshape_U_permuted`\n",
    "        # (b * patches, patch_size) -> (b, patches, patch_size)\n",
    "        U_permuted_grad = U_reshaped_grad.reshape(b, patches, patch_size)\n",
    "        assert U_permuted_grad.shape == (b, patches, patch_size)\n",
    "\n",
    "        # backward of `permute_U`\n",
    "        # (b, patches, patch_size) -> (b, patch_size, patches)\n",
    "        U_grad = U_permuted_grad.permute(0, 2, 1)\n",
    "        assert U_grad.shape == (b, patch_size, patches)\n",
    "\n",
    "        # backward of `unfold`\n",
    "        torch_fold = torch.nn.Fold(\n",
    "            output_size=(i_h, i_w),\n",
    "            kernel_size=(kernel_h, kernel_w),\n",
    "            padding=padding,\n",
    "            stride=stride,\n",
    "        )\n",
    "        input_batch_grad = torch_fold(U_grad)\n",
    "\n",
    "        return input_batch_grad, kernel_grad, None, None\n",
    "\n",
    "\n",
    "# Initialize input tensors --------\n",
    "torch.manual_seed(42)\n",
    "input_batch = torch.randn(batches, in_channels, i_h, i_w).requires_grad_(True)\n",
    "kernel = torch.randn(out_channels, in_channels, k_h, k_w).requires_grad_(True)\n",
    "\n",
    "# Verify output --------\n",
    "ctx, custom_conv2d_output = Conv2DFunc.apply(input_batch, kernel, stride, padding)\n",
    "\n",
    "torch_conv2d_output = torch.nn.functional.conv2d(\n",
    "    input=input_batch, weight=kernel, stride=stride, padding=padding\n",
    ")\n",
    "torch.testing.assert_allclose(custom_conv2d_output, torch_conv2d_output)\n",
    "\n",
    "# Verify gradients --------\n",
    "grad_output = torch.randn_like(custom_conv2d_output, requires_grad=True)\n",
    "torch_conv2d_output.backward(grad_output)\n",
    "\n",
    "custom_input_batch_grad, custom_kernel_grad, _, _ = Conv2DFunc.backward(\n",
    "    ctx, grad_output\n",
    ")\n",
    "\n",
    "torch.testing.assert_allclose(custom_kernel_grad, kernel.grad)\n",
    "torch.testing.assert_allclose(custom_input_batch_grad, input_batch.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
